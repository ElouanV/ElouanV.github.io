<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>Choix du Modèle</title>
    <link rel="stylesheet" href="../styles.css">

</head>
<body>

<header>
    <h1>Choix du Modèle</h1>
    <nav>
        <a href="index.html">Retour à l'accueil</a>
    </nav>
</header>

<main>
    <p>On l'a évoqué dans d'autres sections, le choix du modèle, de son architecture et de ses hyperparamètres vont forcément impacter les ressources, dont le modèle à besoin.
        Evidemment, ChatGPT consomme un peu plus qu'un modèle de classification de chiffre en noir et blanc. De la même manière, un CNN avec 100 couches consomme a priori moins d'un CNN à trois couches.
        En général, le nombre de paramètres d'un modèle est un bon indicateur de sa complexité, mais ce n'est pas tout le temps vrai. Certaines architectures, même si elles ont moins de paramètre, de par leur fonctionnement, sont plus gourmande.
        Le choix du modèle impact aussi le nombre de fois que le modèle va devoir "voir" les données, et donc potentiellement son temps d'entraînement, et donc les ressources nécessaires à son entraînement.
        En effet, certains modèles vont converger bien plus vite que d'autres, et ça peut dépendre du problème auquel on est confronté.
    </p>
    <p>
        D'autre part, regarder uniquement le temps d'entraînement, le taille du modèle, il faut aussi prendre en compte le temps d'inférence, sinon les comparaisons ne sont pas juste.
        Si on prend l'exemple d'un KNN, dont le temps d'entraînement est de 0 et qu'on le compare au temps d'entraînement d'un modèle profond, la comparaison n'est pas juste. Le KNN va avoir un temps d'inférence qui peut être énorme en fonction de la taille du jeu de données et du type de donnée...
        Sans parler de la scalabilité du KNN qui le rend inutilisable dans la majorité des cas...
    </p>
    <p> 
        Donc, on ne peut pas comparer les modèles uniquement sur le temps d'entraînement, ni sur la métrique de performance, ni sur leur taille, ni sur leur nombre de paramètres.
        Prise une par une ces métriques peuvent être facile à maximiser pour un modèle, le but du sujet qu'on aborde dans cette veille n'est pas d'avoir le modèle le moins "lourd" sur tous les plans, mais de trouver le bon équilibre.
    </p>
    <p>  
        On recherche donc un modèle scalable, qui s'entraîne relativement vite avec des ressources raisonnables, mais aussi qui a de bonnes performances.
    </p>
    <p> 
        Tous ceux qui ont déjà eu à entraîner un modèle savent que pour choisir le meilleur modèle avec les meilleurs hyperparamètres pour un problème donnée qui maximise la métrique de performance peut vite devenir pénible.
        Alors imaginer quand en plus de ça, on doit prendre en compte d'autre métrique qui caractérisent l'efficacité d'un modèle.
    </p>
    <p> 
        Une fois le problème énoncé, on se rend rapidement compte que l'on va avoir du mal à trouver le modèle parfait.
        Evidemment, on pourrait penser que la solution, c'est de définir une métrique globale, qui prend en compte tous nos objectifs, d'entraîner plein de modèles différents, en essayant plusieurs combinaisons d'hyperparamètres sur chaque modèle, et de prendre le meilleur de tous.
    </p>
    <p> 
        Dans la suite de cette section de notre mindmap, on discutera un peu de ce qui peut nous aider à choisir mieux nos modèles.
    </p>
    <p> 
        A noter que dans cette veille, nous ne nous étalerons pas trop sur le sujet des choix des modèles, c'est la problématique abordée par un étudiant dans le même cadre, qui étudie les alternatives au deep learning et l'intérêt de choisir un modèle pertinent pour chaque problème.
        Pour creuser un peu plus le sujet, vous pouvez consulter la ressource qu'il a produite (liens à venir).
    </p>
    <!-- Ajoutez ici vos explications sur le choix du modèle -->
</main>

<footer>
    <p>&copy; 2024 Elouan Vincent</p>
</footer>

</body>
</html>
