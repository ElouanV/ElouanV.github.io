<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>Transfer learning</title>
    <link rel="stylesheet" href="../styles.css">

</head>
<body>

<header>
    <h1>Bibliographie</h1>
    <nav>
        <a href="index.html">Retour à la mindmap</a>
    </nav>
</header>

<main>
    <p>
        Sources :
        <ul>
            <li><a href="https://ecoinfo.cnrs.fr/2010/10/20/le-silicium-la-fabrication/">site</a></li>
            <li><a href="https://www.trgdatacenters.com/resource/ai-chatbots-energy-usage-of-2023s-most-popular-chatbots-so-far/#:~:text=ChatGPT%3A%20Energy%20Consumption%20in%20MWh%20%3D%201%2C248%20MWh&text=OpenAI's%20documentation%20states%20that%20training,%2Fs%2Ddays%20of%20computation.">TRGDatacenters</a></li>
            <li><a href="https://medium.com/@zodhyatech/how-much-energy-does-chatgpt-consume-4cba1a7aef85">Medium</a>            </li>
            <li><a href="https://www.carbone4.com/myco2-empreinte-moyenne-evolution-methodo">Carbon4</a></li>
            <li><a href="https://www.statistiques.developpement-durable.gouv.fr/lempreinte-carbone-de-la-france-de-1995-2021">Ministère de la Transition énergétique</a>            </li>
            <li><a href="https://dl.acm.org/doi/pdf/10.1145/3381831">Efficiency VS Performance</a></li>
            <li>MEHLIN, Vanessa, SCHACHT, Sigurd, et LANQUILLON, Carsten. Towards energy-efficient Deep Learning: An overview of energy-efficient approaches along the Deep Learning Lifecycle. arXiv preprint arXiv:2303.01980, 2023. <a href="https://arxiv.org/pdf/2303.01980.pdf">papier</a></li>
            <li>M. Shahshahani and D. Bhatia, "Resource and Performance Estimation for CNN Models using Machine Learning," 2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), Tampa, FL, USA, 2021, pp. 43-48, doi: 10.1109/ISVLSI51109.2021.00019.<a href="https://ieeexplore.ieee.org/document/9516802">papier</a></li>
            <li><a href="https://medium.com/@souvik.paul01/pruning-in-deep-learning-models-1067a19acd89">Medium sur le model pruning</a></li>
            <li>HOEFLER, Torsten, ALISTARH, Dan, BEN-NUN, Tal, et al. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. The Journal of Machine Learning Research, 2021, vol. 22, no 1, p. 10882-11005.<a href="https://www.jmlr.org/papers/volume22/21-0366/21-0366.pdf">papier sur le pruning</a></li>
            <li>ZHU, Michael et GUPTA, Suyog. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017. <a href="https://arxiv.org/pdf/1710.01878.pdf">papier sur le pruning</a></li>
            <li>POLINO, Antonio, PASCANU, Razvan, et ALISTARH, Dan. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668, 2018.<a href="https://arxiv.org/pdf/1802.05668"> papier sur la quantization</a></li>
            <li>KLIMCZAK, Adrianna, WENKA, Marcel, GANZHA, Maria, et al. Towards Frugal Artificial Intelligence: Exploring Neural Network Pruning and Binarization. In : International Symposium on Intelligent Informatics. Singapore : Springer Nature Singapore, 2022. p. 13-27. <a href="https://link.springer.com/chapter/10.1007/978-981-19-8094-7_2">papier</a></li>
            <li>ZHANG, Chen, XIE, Yu, BAI, Hang, et al. A survey on federated learning. Knowledge-Based Systems, 2021, vol. 216, p. 106775.<a href="https://www.sciencedirect.com/science/article/pii/S0950705121000381">papier</a></li>
            <li>LI, Li, FAN, Yuxi, TSE, Mike, et al. A review of applications in federated learning. Computers & Industrial Engineering, 2020, vol. 149, p. 106854.<a href="https://orca.cardiff.ac.uk/id/eprint/134968/1/CAIE_A%20review%20of%20applications%20in%20federated%20learning_deposit.pdf">papier</a></li>
            <li>JIANG, Yuang, WANG, Shiqiang, VALLS, Victor, et al. Model pruning enables efficient federated learning on edge devices. IEEE Transactions on Neural Networks and Learning Systems, 2022.<a href="https://ieeexplore.ieee.org/iel7/5962385/6104215/09762360.pdf?casa_token=HHp5Ezerf8AAAAAA:EPN923j-b3r5ZKKMkah2tTUTHbI5YogJPRa6PcTEUQN8qyr2BPCnXudaNWXjDuKXshmo9YS8_IN27g">papier</a></li>
            <li>WEISS, Karl, KHOSHGOFTAAR, Taghi M., et WANG, DingDing. A survey of transfer learning. Journal of Big data, 2016, vol. 3, no 1, p. 1-40. <a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6">papier</a></li>
            <li><a href="https://datascientest.com/transfer-learning">source de l'image ci-dessus</a></li>  
        </ul>
    </p>

    <p>
        Outils: 
        <ul>
            <li>eco2AI: S. Budennyy et al., <l>Eco2AI: carbon emissions tracking of machine learning models as the first
                step towards sustainable AI</l>. arXiv, Aug. 03, 2022. Accessed: Sep. 26, 2022. [Online].
                Available: <a href="http://arxiv.org/abs/2208.00406">papier</a></li>
            <li>Carbontracker: L. F. W. Anthony, B. Kanding, and R. Selvan, <l>Carbontracker: Tracking and Predicting the
                Carbon Footprint of Training Deep Learning Models</l>, ArXiv200703051 Cs Eess Stat, Jul. 2020,
                Accessed: May 10, 2022. [Online]. Available: <a href="http://arxiv.org/abs/2007.03051">papier</a></li>
            <li>CodeCarbon: . Schmidt et al., mlco2/codecarbon: v2.1.4. Zenodo, 2022. doi: 10.5281/ZENODO.4658424.</li>
            <li>Energy Usage Reports: K. Lottick, S. Susai, S. A. Friedler, and J. P. Wilson, <l>Energy Usage Reports: Environmental
                awareness as part of algorithmic accountability</l>, ArXiv Learn., Nov. 2019 </li>
            <li>EnergyVis: O. Shaikh et al., <l>EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML
                Models</l>, ArXiv Learn., 2021</li>
            <li>Experiment-Impact-Tracker: P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau, <l>Towards the Systematic
                Reporting of the Energy and Carbon Footprints of Machine Learning</l>, Jan. 2020. [Online].
                Available: <a href="http://arxiv.org/pdf/2002.05651v1">papier</a></li>
            <li>Green Algorithms: L. Lannelongue, J. Grealey, and M. Inouye, <l>Green Algorithms: Quantifying the carbon
                emissions of computation.</l>, Jul. 2020.</li>
            <li>ML CO2 Impact: A. Lacoste, Alexandra Luccioni, A. Luccioni, V. Schmidt, and T. Dandres, <l>Quantifying the
                Carbon Emissions of Machine Learning.</l>, ArXiv Comput. Soc., Oct. 2019.</li>
        </ul> 
    </p>
    
</main>

<footer>
    <p>&copy; 2024 Elouan Vincent</p>
</footer>

</body>
</html>